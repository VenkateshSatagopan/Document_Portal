{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all_ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user is asking, \"What is the capital of Germany?\" Let me start by recalling what I know about Germany.\\n\\nFirst, I remember that Germany is a country in Europe. I think it\\'s in the central part of the continent. Now, capitals of countries... I know that some big cities in Germany are Berlin, Munich, Hamburg, Frankfurt, maybe Cologne. Wait, but which one is the capital?\\n\\nI\\'m pretty sure Berlin is the capital. But why do I think that? I remember that after the reunification of East and West Germany in 1990, the capital was moved from Bonn to Berlin. Before that, Bonn was the capital of West Germany, right? So after the fall of the Berlin Wall, they decided to move the capital back to Berlin. That makes sense because Berlin is a major city and historically significant.\\n\\nLet me verify this. If I think about other countries in Europe, like France\\'s capital is Paris, Italy\\'s is Rome, so Germany\\'s should be Berlin. But wait, sometimes people confuse Berlin with other cities. Maybe someone might think Munich because that\\'s where the Oktoberfest is, but no, Munich is in Bavaria. Frankfurt is a big financial hub, but not the capital. Hamburg is a major port city. So yeah, Berlin is the capital.\\n\\nAlso, I can recall that the German parliament is in Berlin, and the government offices are there. So that\\'s another clue. If I\\'m not 100% sure, maybe I can think of other sources. For example, in the news, when they talk about Germany\\'s government, they mention Berlin. So yes, it\\'s safe to say Berlin is the capital. I think that\\'s correct. Let me just make sure there\\'s no confusion with other cities. No, I think that\\'s right. Berlin is the capital of Germany.\\n</think>\\n\\nThe capital of Germany is **Berlin**. \\n\\nKey points to note:\\n- **Historical Context**: After Germany\\'s reunification in 1990, Berlin replaced Bonn as the capital (Bonn had been the capital of West Germany during the division).\\n- **Political Significance**: Berlin is home to Germany\\'s federal government, including the Bundestag (parliament) and the Chancellery.\\n- **Cultural Hub**: A major European city, Berlin is known for its rich history, vibrant arts scene, and landmarks like the Berlin Wall and Brandenburg Gate.\\n\\nBerlin became the capital in 1990, symbolizing the reunification of East and West Germany.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is capital of Germany\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02519318461418152, -0.045027073472738266, -0.03575272858142853, -0.012785417027771473, 0.04907818138599396, 0.004383859224617481, -0.01205921359360218, -0.02788645587861538, 0.02122546173632145, 0.07602448761463165, -0.010635130107402802, -0.001221328042447567, -0.009708001278340816, 0.024484600871801376, 0.005975647363811731, -0.01022539846599102, -0.003973711282014847, 0.05629425868391991, 0.0297442264854908, -0.02412020042538643, -0.0026169661432504654, 0.006723635829985142, -0.01645795814692974, 0.010868522338569164, 0.03220163658261299, -0.0314304456114769, -0.01233163382858038, -0.052722349762916565, 0.004460289143025875, -0.009758503176271915, -0.07167020440101624, 0.009074417874217033, 0.006552789825946093, 0.00423111068084836, 0.01168059092015028, -0.049213625490665436, -0.015349424444139004, -0.0055377488024532795, -0.028519107028841972, 0.06603482365608215, -0.014149638824164867, -0.04218465834856033, -0.06554737687110901, -0.0017994308145716786, -0.0038072289898991585, -0.019856562837958336, -0.028126586228609085, 0.02351042628288269, 0.04600263014435768, -0.03204841911792755, 0.005496886093169451, -0.006000625900924206, 0.053449153900146484, 0.010093560442328453, 0.007096298970282078, -0.0676269382238388, -0.0037817503325641155, -0.026787178590893745, 0.021233009174466133, 0.03070538304746151, 0.027800239622592926, -0.032930780202150345, -0.0018899189308285713, 0.05134987458586693, -0.006661455146968365, -0.0680573582649231, 0.003130798228085041, 0.025607943534851074, 0.08795329183340073, -0.0010102253872901201, -0.012268186546862125, -0.042318783700466156, 0.06099323928356171, -0.012085274793207645, 0.009020738303661346, -0.07269164174795151, -0.026529310271143913, 0.06357596814632416, -0.008029366843402386, -0.029836630448698997, 0.010094936937093735, -0.047152329236269, -0.07491640001535416, -0.02275633066892624, -0.05469963327050209, 0.019607292488217354, -0.04974066838622093, -0.003183363238349557, -0.033595841377973557, 0.057950709015131, -0.02129148133099079, -0.003980857785791159, 0.05324753373861313, -0.04917323589324951, -0.03355741500854492, 0.09252414107322693, -0.0062112328596413136, -0.027816658839583397, 0.05394841730594635, -0.025718925520777702, 0.026179159060120583, -0.05073793604969978, -0.08076443523168564, 0.04414790868759155, 0.04101988300681114, 0.03612053021788597, 0.004439949989318848, 0.027806371450424194, -0.0021066695917397738, 0.04072166979312897, -0.062373608350753784, -0.03223505616188049, 0.0582660548388958, 0.04473065212368965, 0.008662897162139416, -0.05519664287567139, -0.025369051843881607, -0.007753746584057808, 0.03084089793264866, 0.014364241622388363, 0.03582902252674103, -0.005269909277558327, 0.03630109876394272, -0.06607840955257416, 0.017509333789348602, -0.022482968866825104, -0.043679360300302505, 0.02349582128226757, -0.038662608712911606, 0.03359047695994377, 0.03492865338921547, -0.05010364577174187, -0.027455877512693405, 0.02390240877866745, -0.010404021479189396, 0.053733691573143005, 0.03639281168580055, -0.04018797725439072, 0.024025971069931984, 0.0008419554214924574, 0.027521908283233643, 0.0630197674036026, 0.008766607381403446, 0.05517964065074921, 0.0018157975282520056, 0.05906665325164795, -0.03776019439101219, -0.037087924778461456, 0.050028227269649506, -0.05826065316796303, -0.061449553817510605, -0.026300009340047836, -0.04294991493225098, -0.02730192057788372, 0.06473159790039062, 0.0226538497954607, -0.027772458270192146, 0.03701794892549515, 0.042985204607248306, -0.010602710768580437, 0.035457268357276917, 0.013863146305084229, 0.007022922392934561, 0.006302850320935249, -0.012040404602885246, -0.01965443417429924, 0.004471755586564541, -0.007465498987585306, 0.02595207281410694, 0.00013234041398391128, -0.03093591332435608, -0.00764697277918458, -0.0001474705641157925, -0.03261655569076538, 0.015609062276780605, -0.038691196590662, 0.003947954624891281, 0.0034531201235949993, -0.04329495131969452, 0.002650747075676918, 0.014884822070598602, -0.05365189537405968, -0.0049507953226566315, 0.03603871166706085, -0.008394169621169567, -0.01316535659134388, 0.04354797303676605, -0.00803719274699688, -0.0024930189829319715, 0.020325208082795143, -0.045333411544561386, 0.011685410514473915, -0.021516935899853706, -0.023026004433631897, -0.04718588665127754, -0.01567457988858223, 0.014779728837311268, -0.024568792432546616, 0.0195423886179924, -0.06335657835006714, -0.03774818405508995, 0.05989925563335419, 0.011557717807590961, -0.018800579011440277, 0.014770837500691414, 0.010956035926938057, 0.08794909715652466, -0.028880799189209938, -0.009991899132728577, 0.02756909653544426, -0.02776537649333477, 0.01184762455523014, 0.0009619502234272659, -0.0056409286335110664, 0.04828720539808273, -0.029007047414779663, 0.06985120475292206, -0.0031095119193196297, 0.037713028490543365, -0.013115109875798225, -0.03171126917004585, 0.019599368795752525, -0.02072761580348015, -0.029738934710621834, -0.004166298545897007, 0.027463354170322418, -0.02118169330060482, -0.02527989074587822, 0.02000059187412262, -0.02115700952708721, -0.024638008326292038, 0.07085751742124557, 0.03214607387781143, -0.004925320856273174, 0.04836396500468254, 0.042860280722379684, -0.0008858012733981013, 0.018131792545318604, -0.001941577298566699, -0.002771385945379734, -0.03700374811887741, -0.025128604844212532, 0.048406802117824554, 0.03178984671831131, -0.04029950127005577, -0.0532713308930397, -0.018981246277689934, 0.0496305413544178, 0.038293756544589996, 0.07024223357439041, 0.016542715951800346, 0.009214088320732117, 0.01768667809665203, 0.02777610719203949, -0.04893937706947327, 0.006146049592643976, -0.05011777952313423, 0.02221519500017166, -0.019901217892766, 0.02812929078936577, 0.026880821213126183, 0.036432843655347824, 0.04629123583436012, -0.045267827808856964, 0.003166912356391549, -0.026904910802841187, 0.008424066938459873, -0.04486424848437309, 0.004640521481633186, -0.006698267534375191, -0.0038068981375545263, -0.09240786731243134, 0.007529283408075571, 0.04061164706945419, 0.015318592078983784, 0.00475598918274045, -0.03739171102643013, 0.05437684431672096, 0.04385439679026604, -0.06653847545385361, 0.05358793959021568, 0.04012583568692207, -0.002583960071206093, -0.020231761038303375, -0.010095731355249882, -0.007908797822892666, -0.057109154760837555, -0.008054202422499657, -0.02502686157822609, -0.048005010932683945, -0.05022536218166351, -0.07206215709447861, -0.006624279078096151, -0.05041259527206421, -0.0880640372633934, -0.018956799060106277, -0.034204285591840744, 0.034498389810323715, 0.03835649415850639, -0.017750168219208717, -0.05188824236392975, -0.03163331747055054, 0.023373158648610115, -0.07596603780984879, 0.008814455941319466, 0.011021329089999199, -0.05160645395517349, -0.07358942925930023, 0.0421375073492527, 0.05909984931349754, 0.02976803295314312, -0.021373579278588295, -0.059007614850997925, -0.010745878331363201, -0.0163129810243845, 0.052054304629564285, -0.037103284150362015, -0.0024928150232881308, -0.008841992355883121, 0.04163970798254013, 0.007444331888109446, 0.08712676167488098, 0.02121850848197937, -0.047607917338609695, -0.002967018634080887, 0.05420941859483719, -0.00022651445760857314, 0.02661711350083351, 0.00010629557800712064, 0.013384870253503323, -0.01913711428642273, 0.03129062056541443, -0.045803945511579514, 0.03096846304833889, 0.019262902438640594, 0.04951966181397438, -0.04215729609131813, 0.039088595658540726, -0.06378386914730072, 0.019140608608722687, 0.07515360414981842, 0.046381715685129166, -0.013747062534093857, -0.05166228488087654, -0.001267984276637435, -0.0008957312675192952, -0.0335221141576767, 0.02062389627099037, 0.07280959188938141, 0.02468794584274292, 0.011226538568735123, 0.10136803984642029, -0.0010301937581971288, 0.04143378138542175, -0.01020901557058096, -0.03530903160572052, 0.05091361701488495, -0.013598459772765636, 0.02154541015625, -0.04959079250693321, -0.015810709446668625, 0.0014217128045856953, -0.018910538405179977, -0.05038013681769371, -0.022964894771575928, -0.03447970747947693, -0.02468123845756054, 0.014912792481482029, 0.03077191300690174, 0.016415810212492943, 0.04279603436589241, 0.02917809784412384, 0.019711531698703766, -0.03227955102920532, 0.04370936006307602, 0.03045501559972763, -0.06470760703086853, -0.010543696582317352, -0.0074870227836072445, 0.014578556641936302, -0.02851261757314205, 0.0036940090358257294, 0.06990644335746765, 0.01710536517202854, 0.01634852960705757, -0.02208075486123562, 0.01300570834428072, 0.04119779169559479, 0.001529346569441259, 0.06498688459396362, -0.04899454116821289, 0.06839204579591751, 0.06381542980670929, -0.010798637755215168, -0.007110896520316601, -0.03239544481039047, 0.0008430212619714439, -0.011207390576601028, 0.024408886209130287, 0.0017393502639606595, -0.0009652832522988319, -0.05991554632782936, 0.011950848624110222, -0.022701432928442955, -0.036736927926540375, -0.004211114719510078, -0.004664641804993153, -0.0010654246434569359, -0.02713918872177601, 0.02116195112466812, 0.004791703540831804, -0.030677180737257004, 0.012464523315429688, -0.04272962361574173, -0.054994042962789536, -0.011488888412714005, 0.010544572956860065, -0.03304775804281235, 0.01347693707793951, 0.08074671030044556, -0.0246442724019289, -0.002452823333442211, 0.005545430816709995, -0.00441216304898262, -0.06577955186367035, -0.06053399294614792, 0.00327476323582232, 0.026256520301103592, 0.037390463054180145, -0.001910378341563046, 0.02875785157084465, 0.006706661079078913, -0.05631734058260918, -0.04998977854847908, -0.039755843579769135, -0.05328827351331711, 0.01148927677422762, 0.03196237236261368, -0.012170454487204552, 0.037264250218868256, -0.005124903284013271, -0.011992136016488075, 0.037593092769384384, 0.029394542798399925, -0.02747785486280918, -0.0016265494050458074, -0.011327648535370827, -0.027133507654070854, 0.014148605056107044, -0.04579346999526024, 0.027582010254263878, -0.0393114909529686, -0.003481467254459858, -0.026261329650878906, -0.09058890491724014, -0.013862404972314835, 0.0008233471889980137, 0.04760856181383133, -0.05430479720234871, 0.04552752524614334, 0.020703066140413284, 0.01118931919336319, -0.004237988032400608, -0.10102365911006927, 0.027811428532004356, 0.029258182272315025, 0.0018480902072042227, 0.004881983157247305, 0.016324728727340698, 0.004301734268665314, 0.021579738706350327, 0.01669153943657875, -0.004653653129935265, -0.02228553779423237, -0.00990954041481018, -0.003517327830195427, -0.049350522458553314, 0.029412435367703438, -0.07187221199274063, 0.016690749675035477, 0.017808545380830765, 0.0037284386344254017, 0.027574487030506134, 0.032382186502218246, -0.023551099002361298, 0.03867310285568237, -0.006593790836632252, -0.012446621432900429, -0.043936993926763535, 0.061603039503097534, 0.007205056957900524, -0.03682126849889755, -0.023950304836034775, -0.02117910422384739, -0.03717988356947899, -0.026783090084791183, 0.030252698808908463, 0.04071471467614174, 0.0005039430106990039, 0.01052966620773077, -0.009641198441386223, -0.02976514957845211, -0.002079461235553026, -0.01883954368531704, 0.0759596899151802, -0.0809766948223114, -0.03143049776554108, 0.009226660244166851, -0.017410948872566223, -0.014661927707493305, 0.011553659103810787, -0.022813821211457253, -0.004518803209066391, 0.02780199982225895, -0.009645719081163406, 0.017117265611886978, 0.013681129552423954, 0.013168834149837494, 0.06013278290629387, 0.006789353210479021, 0.014648867771029472, -0.00504711177200079, -0.08768274635076523, 0.00454753590747714, -0.03446483239531517, 0.011321214959025383, 0.03997889161109924, 0.04481784999370575, -0.06911914050579071, 0.0006408229819498956, -0.009105341508984566, 0.009026876650750637, -0.024595297873020172, -0.02350773476064205, 0.012207293882966042, -0.017146313562989235, 0.0058694095350801945, 0.024315619841217995, 0.04142323508858681, -0.03969379886984825, 0.04734454303979874, -0.017734138295054436, 0.007306267973035574, 0.026101527735590935, -0.056092601269483566, 0.016131114214658737, 0.0019438461167737842, -0.04008778557181358, 0.0005296398885548115, 0.03856787085533142, -0.01837063767015934, 0.007349551655352116, 0.015577179379761219, -0.03373667970299721, -0.00022010186512488872, -0.023845920339226723, -0.007446296513080597, 0.02987445332109928, -0.012064381502568722, -0.04234451428055763, -0.05901718884706497, -0.01427498459815979, -0.00038812457933090627, 0.017468754202127457, 0.08101721107959747, 0.04928067699074745, -0.008775794878602028, -0.021363137289881706, 0.04752415418624878, -0.024456189945340157, -0.005221992265433073, -0.017100689932703972, 0.05230750888586044, 0.06132500246167183, 0.03614002838730812, 0.0024165185168385506, -0.022505059838294983, -0.02878936193883419, -0.040689632296562195, -0.02959349937736988, 0.053134553134441376, -0.0010008462704718113, 0.003509376896545291, 0.03767534717917442, -0.002037706086412072, 0.048874251544475555, 0.059903066605329514, 0.09170889109373093, 0.026000889018177986, 0.05115010216832161, -0.018270185217261314, 0.008100606501102448, -0.023830236867070198, 0.023344095796346664, 0.04540765658020973, 0.004543566145002842, -0.023239169269800186, -0.0163746178150177, -0.00446572108194232, 0.00020715179562103003, 0.053405728191137314, -0.03131946548819542, 0.001604071818292141, 0.013611899688839912, -0.015112400986254215, 0.0681137889623642, -0.01970621384680271, 0.01821029931306839, -0.0025327319744974375, 0.023415744304656982, 0.008740968070924282, -0.031593989580869675, 0.02069285698235035, -0.03085978515446186, -0.04326770082116127, -0.0010377410799264908, 0.07605105638504028, -0.02507494017481804, 0.007005617488175631, -0.031935229897499084, -0.007331180851906538, 0.017733244225382805, 0.00039335666224360466, -0.02824464999139309, 0.012529945932328701, 0.033252689987421036, 0.013981053605675697, -0.0028140973299741745, 0.07595006376504898, 0.006868560798466206, 0.06157079339027405, 0.04034397751092911, -0.030472079291939735, 0.012216692790389061, -0.04674750193953514, -0.020043283700942993, -0.05225978046655655, 0.011758913286030293, 0.008937829174101353, -0.036475371569395065, -0.0420452319085598, -0.016633301973342896, 0.002191652311012149, 0.008256220258772373, 0.027214467525482178, 0.13491302728652954, 0.03161618858575821, -0.07433187961578369, -0.04068341478705406, -0.012691042385995388, -0.02210696041584015, -0.00449434295296669, 0.011145367287099361, -0.03015080839395523, 0.04523959755897522, -0.022893991321325302, -0.04648866131901741, -0.038739919662475586, 0.034831032156944275, 0.007287988439202309, -0.024318557232618332, -0.01469244435429573, -0.01092138234525919, 0.002725916216149926, -0.026452574878931046, -0.001583313336595893, -0.018230047076940536, -0.094106525182724, -0.021618414670228958, 0.05578935146331787, -0.031283650547266006, 0.0068091703578829765, -0.005951364059001207, -0.013745413161814213, 0.0529613234102726, 0.01836998201906681, -0.013553747907280922, -0.01753789559006691, 0.023710548877716064, 0.027613798156380653, 0.021482840180397034, 0.019351527094841003, -0.03205036371946335, 0.041006285697221756, -0.03900200128555298, 0.04107536002993584, 0.011040943674743176, 0.02975563332438469, -0.023286247625947, -0.03788702189922333, 0.00987574178725481, 0.008304519578814507, -0.06347890198230743, 0.032140158116817474, 0.09797337651252747, 0.029059424996376038, 0.02412491850554943, 0.00954884197562933, 0.01919691450893879, -0.011723387986421585, -0.03528817370533943, -0.029782362282276154, -0.021574238315224648, 0.013563486747443676, 0.03277361020445824, 0.013188167475163937, 0.030875282362103462, 0.05744514986872673, 0.046885352581739426, 0.05159841850399971, 0.031999051570892334, -0.046662162989377975, -0.025843903422355652, 0.024442629888653755, 0.016933636739850044, -0.03995649144053459, 0.011383624747395515, 0.02468216046690941, -0.011965095065534115, 0.05711297690868378, 0.08032999187707901, 0.024698758497834206, 0.050999417901039124, -0.020708473399281502, -0.10388366878032684, 0.07284938544034958, -0.06790672987699509, -0.016249949112534523, -0.03594491630792618, 0.02893761731684208, 0.08671095967292786, 0.007472796831279993, 0.002856259932741523, -0.003249375382438302, -0.010087274014949799, 0.11344458907842636, 0.03259596228599548, 0.06144828721880913, 0.0272892527282238, -0.09670468419790268, -0.01991722173988819, 0.028650574386119843, -0.015925785526633263, 0.025446563959121704, 0.021593190729618073, -0.02600196562707424, -0.0433976911008358, -0.033136531710624695, -0.01149297971278429, -0.06949947774410248, -0.03352520242333412, -0.0018494041869416833, -0.028463173657655716, 0.060790982097387314, 0.047520287334918976, 0.0032247984781861305, -0.02566937357187271, -0.013506890274584293, -0.009559150785207748, -0.009393188171088696, -0.007347462698817253, -0.0429837703704834, -0.010548611171543598, 0.00646340474486351, -0.006682183127850294, 0.01463911309838295, 0.020520873367786407, -0.0018707517301663756]\n"
     ]
    }
   ],
   "source": [
    "response = embedding_model.embed_query(\n",
    "  \"What is the capital of Germany\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # Number of characters to be present in chunk\n",
    "    chunk_overlap=150, # Character overlap between 2 chunks\n",
    "    length_function=len # Paramter to calculate length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'd:\\\\LLMOPS\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedd_vector = embedding_model.embed_documents(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_embedd_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Faiss can be in memory or in disk storage, cloud storage not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc= vectorstore.similarity_search(\"what is a llama llm model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc= vectorstore.similarity_search(\"what is a llama llm model?\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 Pretraining\\nTocreatethenewfamilyof Llama 2models,webeganwiththepretrainingapproachdescribedinTouvronetal.\\n(2023), using an optimized auto-regressive transformer, but made several changes to improve performance.\\nSpecifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total\\ntokens,doubledthecontextlength,andusedgrouped-queryattention(GQA)toimproveinferencescalability'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[9].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='88842cb4-982d-48ea-850e-2f23cf1b7a49', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\LLMOPS\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='3a18ada0-cef1-42ac-9c8f-4b807c0ce6ca', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\LLMOPS\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='ba64c10a-8c54-48ef-b863-1a901773dc40', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\LLMOPS\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='4022ea57-cff3-414a-9501-2ace7b2a96cc', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\LLMOPS\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama funetuning benchmark experiments/ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the question based on the context provided beloe.\n",
    "If the content does not contain sufficient information , respond with:\n",
    "\" I donot have enough information about this.\"\n",
    "Context: {context} \n",
    "Question : {question}\n",
    "Answer : \"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s try to figure out the answer to the question about the Llama 2 fine-tuning benchmark experiments based on the provided context.\\n\\nFirst, I need to understand what the context is showing. There are tables with different model sizes (like 7B, 13B, 34B, 70B) for Llama 2 and other models. The numbers might represent performance metrics on various benchmarks. The user is asking specifically about the fine-tuning experiments for Llama 2. \\n\\nLooking at the context, there\\'s a section labeled \"Fine-tuned\" under the Llama 2 entries. The numbers there could be the results from those fine-tuning experiments. However, the numbers are a bit jumbled and not clearly labeled. For example, under \"Fine-tuned\" there\\'s a line starting with \"ChatGPT\" with several decimal numbers, followed by more numbers for Llama 2\\'s different model sizes. The numbers might correspond to different benchmarks or metrics, but the exact meaning isn\\'t clear without column headers.\\n\\nThe context mentions that Table 3 summarizes overall performance across benchmarks grouped into categories, and individual results are in Section A.2.2. However, the user\\'s context doesn\\'t include that section. Also, the numbers provided are just a list without clear labels for what each column represents. For instance, the first line after \"Fine-tuned\" has numbers like 0.23, 0.22, etc., but without knowing which benchmark each number corresponds to, it\\'s hard to interpret.\\n\\nSince the user is asking for details on the fine-tuning experiments, but the context lacks proper labels or explanations for the numbers, I can\\'t determine specific results or conclusions. The information provided is insufficient to answer the question accurately. Therefore, the correct response would be to state that there\\'s not enough information available.\\n</think>\\n\\nI donot have enough information about this.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell  me about the llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
